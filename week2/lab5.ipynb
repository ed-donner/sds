{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5\n",
    "\n",
    "Advanced topics with APIs: Scaling\n",
    "\n",
    "## Prompt caching and Batch APIs"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "from dotenv import load_dotenv\n",
    "from litellm import completion\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydantic.*\")\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 2,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak, man.\n",
      "  Laer. Where is my father?\n",
      "  King. Dead.\n",
      "  Queen. But not by him!\n",
      "  King. Let him deman\n"
     ]
    }
   ],
   "source": [
    "with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hamlet = f.read()\n",
    "\n",
    "loc = hamlet.find(\"Speak, man\")\n",
    "print(hamlet[loc:loc+100])"
<<<<<<< Updated upstream
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\""
=======
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< Updated upstream
    "messages = [{\"role\": \"user\", \"content\": question}]\n",
    "response = completion(model=\"gpt-4.1-nano\", messages=messages)\n",
    "reply = response.choices[0].message.content\n",
    "print(reply)"
=======
    "question = \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\""
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Act 4, Scene 5 of *Hamlet*, when Laertes asks \"Where is my father?\", the reply he receives is:\n",
      "\n",
      "**\"At somnorous distillation; / Or, as the lady said, 'Whiles she was coughing at it.'\"**\n",
      "\n",
      "This line is spoken by the First Player, who is reporting the supposed death of Polonius, saying that Polonius is dead and has been laid out. The actual response to Laertes's question about his father’s whereabouts is that Polonius has been killed and is lying in the chapel or in a place of rest, often implied as a funeral or burial site. The scene depicts Laertes’s grief and outrage upon learning of his father's death.\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": question}]\n",
    "response = completion(model=\"gpt-4.1-nano\", messages=messages)\n",
    "reply = response.choices[0].message.content\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 25\n",
      "Output tokens: 148\n",
      "Total cost: 0.0062 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< Updated upstream
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
=======
>>>>>>> Stashed changes
    "system = f\"\"\"\n",
    "You are a helpful assistant that can answer questions about the text of Hamlet.\n",
    "For context, here is the entire text of Hamlet:\n",
    "{hamlet}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Hamlet, when Laertes asks \"Where is my father?\" the reply is \"Dead.\"\n"
     ]
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "messages = [{\"role\":\"system\", \"content\": system}, {\"role\": \"user\", \"content\": question}]\n",
    "response = completion(model=\"gpt-4.1-nano\", messages=messages)\n",
    "reply = response.choices[0].message.content\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 49706\n",
      "Output tokens: 19\n",
      "Cached tokens: 49536\n",
      "Total cost: 0.1263 cents\n"
     ]
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Cached tokens: {response.usage.prompt_tokens_details.cached_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Hamlet, when Laertes asks \"Where is my father?\", the reply is \"Dead.\"\n"
     ]
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "#AGAIN!!\n",
    "\n",
    "response = completion(model=\"gpt-4.1-nano\", messages=messages)\n",
    "reply = response.choices[0].message.content\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 49706\n",
      "Output tokens: 19\n",
      "Cached tokens: 49536\n",
      "Total cost: 0.1263 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Cached tokens: {response.usage.prompt_tokens_details.cached_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< Updated upstream
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Cached tokens: {response.usage.prompt_tokens_details.cached_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
=======
>>>>>>> Stashed changes
    "## Prompt Caching with OpenAI\n",
    "\n",
    "For OpenAI:\n",
    "\n",
    "https://platform.openai.com/docs/guides/prompt-caching\n",
    "\n",
    "> Cache hits are only possible for exact prefix matches within a prompt. To realize caching benefits, place static content like instructions and examples at the beginning of your prompt, and put variable content, such as user-specific information, at the end. This also applies to images and tools, which must be identical between requests.\n",
<<<<<<< Updated upstream
    "\n",
    "\n",
    "Cached input is 4X cheaper\n",
    "\n",
=======
    "\n",
    "\n",
    "Cached input is 4X cheaper\n",
    "\n",
>>>>>>> Stashed changes
    "https://openai.com/api/pricing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Caching with Anthropic\n",
    "\n",
    "https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching\n",
<<<<<<< Updated upstream
    "\n",
    "You have to tell Claude what you are caching\n",
    "\n",
    "You pay 25% MORE to \"prime\" the cache\n",
    "\n",
=======
    "\n",
    "You have to tell Claude what you are caching\n",
    "\n",
    "You pay 25% MORE to \"prime\" the cache\n",
    "\n",
>>>>>>> Stashed changes
    "Then you pay 10X less to reuse from the cache with inputs.\n",
    "\n",
    "https://www.anthropic.com/pricing#api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini supports both 'implicit' and 'explicit' prompt caching\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/caching?lang=python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
